机器:
10.10.96.210 lst
10.10.96.211 lst1
10.10.96.212 lst2

1. 去官网下载最新版本: hadoop-3.1.0.tar.gz
2. tar -zxvf hadoop-3.1.0.tar.gz
3. cd /data/hadoop-3.1.0/etc/hadoop
	--mkdir hdfs/tmp
	--mkdir hdfs/name
	--mkdir hdfs/data
4. 修改 core-site.xml
		<configuration>
		<!-- 指定HDFS老大（namenode）的通信地址 -->
		    <property>
			<name>fs.defaultFS</name>
			<value>hdfs://10.10.96.210:9000</value>
		    </property>
		    <!-- 指定hadoop运行时产生文件的存储路径 -->
		    <property>
			<name>hadoop.tmp.dir</name>
			<value>file:///data/hadoop-3.1.0/hdfs/tmp</value>
		    </property>
		</configuration>

5. 修改 hdfs-site.xml 

		<configuration>
		<!-- 设置namenode的http通讯地址 -->
		    <property>
			<name>dfs.namenode.http-address</name>
			<value>10.10.96.210:50070</value>
		    </property>

		    <!-- 设置secondarynamenode的http通讯地址 -->
		    <property>
			<name>dfs.namenode.secondary.http-address</name>
			<value>10.10.96.210:50090</value>
		    </property>

		    <!-- 设置namenode存放的路径 -->
		    <property>
			<name>dfs.namenode.name.dir</name>
			<value>file:///data/hadoop-3.1.0/hdfs/name</value>
		    </property>

		    <!-- 设置hdfs副本数量 -->
		    <property>
			<name>dfs.replication</name>
			<value>2</value>
		    </property>
		    <!-- 设置datanode存放的路径 -->
		    <property>
			<name>dfs.datanode.data.dir</name>
			<value>file:///data/hadoop-3.1.0/hdfs/data</value>
		    </property>
		    
		    <property>
			<name>dfs.permissions</name>
			<value>false</value>
		    </property>

		</configuration>

6. 修改  yarn-site.xml 
		<configuration>

		<!-- Site specific YARN configuration properties -->
		<property>  
			<name>yarn.nodemanager.aux-services</name>  
			<value>mapreduce_shuffle</value>  
		    </property>  
		    <property>  
			<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>  
			<value>org.apache.hadoop.mapred.ShuffleHandle</value>  
		    </property>  
		    <property>  
			<name>yarn.resourcemanager.resource-tracker.address</name>  
			<value>10.10.96.210:8025</value>  
		    </property>  
		    <property>  
			<name>yarn.resourcemanager.scheduler.address</name>  
			<value>10.10.96.210:8030</value>  
		    </property>  
		    <property>  
			<name>yarn.resourcemanager.address</name>  
			<value>10.10.96.210:8040</value>  
		    </property> 
		</configuration>

7. 修改  mapred-site.xml
		<configuration>
		<!-- 通知框架MR使用YARN -->
		    <property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		    </property>
		    <property>  
			<name>mapreduce.application.classpath</name>  
			<value>  
			/data/hadoop-3.1.0/etc/hadoop,  
			/data/hadoop-3.1.0/share/hadoop/common/*,  
			/data/hadoop-3.1.0/share/hadoop/common/lib/*,  
			/data/hadoop-3.1.0/share/hadoop/hdfs/*,  
			/data/hadoop-3.1.0/share/hadoop/hdfs/lib/*,  
			/data/hadoop-3.1.0/share/hadoop/mapreduce/*,  
			/data/hadoop-3.1.0/share/hadoop/mapreduce/lib/*,  
			/data/hadoop-3.1.0/share/hadoop/yarn/*,  
			/data/hadoop-3.1.0/share/hadoop/yarn/lib/*  
			</value>  
		    </property>
		</configuration>

8. 修改  hadoop-env.sh 
	加入:export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_151

9. 修改  .bashrc 文件  vim ~.bashrc  ==>> source .bashrc
	加入:
	# for examples
	export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_151
	export HADOOP_HOME=/data/hadoop-3.1.0
	export HADOOP_INSTALL=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
	
10. 修改 /etc/profile             ==>> source profile
	在文件末尾添加:
	export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_151
	export JRE_HOME=$JAVA_HOME/jre
	export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
	export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH

11. 启动 hodoop  
- 10.10.96.210 
- cd /data/hadoop-3.1.0/sbin
- hdfs namenode -format
- ./start-all.sh

10.10.96.210
-- ResourceManager
-- SecondaryNameNode
-- NameNode

10.10.96.211
-- DataNode
-- NodeManager

10.10.96.212
-- DataNode
-- NodeManager




